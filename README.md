# Dive into Deep Learning - Notes and Implementations

This repository contains my notes and code implementations from the book [Dive into Deep Learning](https://d2l.ai/). All code is written in Python using PyTorch and Jupyter Notebooks.

## Technologies  
- **Python**  
- **PyTorch**  
- **Jupyter Notebook**  

## Progress  

### 1. Preface  
- [ ] 1.1 Motivation  
- [ ] 1.2 A Taste of Deep Learning  

### 2. Preliminaries  
- [ ] 2.1 Data Manipulation  
- [ ] 2.2 Data Preprocessing  
- [ ] 2.3 Linear Algebra  
- [ ] 2.4 Calculus  
- [ ] 2.5 Automatic Differentiation  
- [ ] 2.6 Probability and Statistics  
- [ ] 2.7 Documentation  

### 3. Linear Neural Networks for Regression  
- [ ] 3.1 Linear Regression  
- [ ] 3.2 The Nature of Linear Regression  
- [ ] 3.3 Linear Regression Implementation from Scratch  
- [ ] 3.4 Concise Implementation of Linear Regression  

### 4. Linear Neural Networks for Classification  
- [ ] 4.1 Softmax Regression  
- [ ] 4.2 Implementation of Softmax Regression from Scratch  
- [ ] 4.3 Concise Implementation of Softmax Regression  

### 5. Multilayer Perceptrons  
- [ ] 5.1 From Linear Regression to Deep Networks  
- [ ] 5.2 Activation Functions  
- [ ] 5.3 Implementation of Multilayer Perceptrons from Scratch  
- [ ] 5.4 Concise Implementation of Multilayer Perceptrons  

### 6. Builderâ€™s Guide  
- [ ] 6.1 Layers and Blocks  
- [ ] 6.2 Parameter Management  
- [ ] 6.3 Deferred Initialization  
- [ ] 6.4 Custom Layers  

### 7. Convolutional Neural Networks  
- [ ] 7.1 From Fully Connected Layers to Convolutions  
- [ ] 7.2 Convolutions for Images  
- [ ] 7.3 Padding and Stride  
- [ ] 7.4 Multiple Input and Output Channels  
- [ ] 7.5 Pooling  

### 8. Modern Convolutional Neural Networks  
- [ ] 8.1 LeNet  
- [ ] 8.2 AlexNet  
- [ ] 8.3 VGG  
- [ ] 8.4 Network in Network (NiN)  
- [ ] 8.5 GoogLeNet  
- [ ] 8.6 Batch Normalization  
- [ ] 8.7 Residual Networks (ResNet)  
- [ ] 8.8 DenseNet  

### 9. Recurrent Neural Networks  
- [ ] 9.1 Sequence Models  
- [ ] 9.2 Text Preprocessing  
- [ ] 9.3 Language Models and the Dataset  
- [ ] 9.4 Recurrent Neural Networks  

### 10. Modern Recurrent Neural Networks  
- [ ] 10.1 Gated Recurrent Units (GRU)  
- [ ] 10.2 Long Short-Term Memory (LSTM)  
- [ ] 10.3 Deep Recurrent Neural Networks  
- [ ] 10.4 Bidirectional RNNs  

### 11. Attention Mechanisms and Transformers  
- [ ] 11.1 Attention Mechanisms  
- [ ] 11.2 Sequence to Sequence Learning  
- [ ] 11.3 Transformers  

### 12. Optimization Algorithms  
- [ ] 12.1 Optimization and Deep Learning  
- [ ] 12.2 Stochastic Gradient Descent  
- [ ] 12.3 Mini-batch Stochastic Gradient Descent  
- [ ] 12.4 Momentum  
- [ ] 12.5 RMSProp  
- [ ] 12.6 Adam  

This list will be updated as I progress.  
